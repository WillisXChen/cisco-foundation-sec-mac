import chainlit as cl
import os
from llama_cpp import Llama
from qdrant_client import QdrantClient

# === Configuration ===
# è¨­å®šé è¨­çš„æ¨¡å‹è·¯å¾‘ï¼Œå¯ä»¥é€éç’°å¢ƒè®Šæ•¸è¦†å¯«
MODEL_SEC_PATH = os.getenv("MODEL_SEC_PATH", "./models/foundation-sec-8b-q4_k_m.gguf")
MODEL_LLAMA3_PATH = os.getenv("MODEL_LLAMA3_PATH", "./models/llama-3-taiwan-8b-instruct-q4_k_m.gguf")

# === å…¨åŸŸè®Šæ•¸ (Global instances) ===
# åªåœ¨å•Ÿå‹•æ™‚è¼‰å…¥ä¸€æ¬¡æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡é€£ç·šéƒ½é‡æ–°è¼‰å…¥æ¶ˆè€—è¨˜æ†¶é«”èˆ‡æ™‚é–“
llm_llama3 = None
llm_sec = None
qdrant_client = None

# === System Messages ===
sec_system_message = (
    "You are Foundation-Sec, a highly advanced cybersecurity, network, server , devops , docker , kubernetes , webserver and system administration expert.\n"
    "RULES:\n"
    "1. Respond directly in English. Do not attempt to translate or use Chinese characters in your output.\n"
    "2. Provide EXACTLY ONE concise paragraph outlining the analysis, root cause, or concept.\n"
    "3. Begin your response immediately with the analysis. Do not echo the user's prompt.\n"
    "4. Do not use markdown headings (#) or numbered lists."
)

general_system_message = (
    "You are a helpful AI assistant. Answer the user's questions politely and naturally in Traditional Chinese."
)

def load_model(model_path: str, context_size: int = 4096):
    """
    Load a Llama-3 based model using llama.cpp with Metal (MPS) support.
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found at {model_path}")

    print(f"Loading model from {model_path}...")
    llm = Llama(
        model_path=model_path,
        n_gpu_layers=-1,      # Metal (GPU) Acceleration on Mac
        seed=1337,            
        n_ctx=context_size,   
        verbose=False,        
        chat_format="llama-3" 
    )
    return llm

@cl.on_chat_start
async def on_chat_start():
    global llm_llama3, llm_sec, qdrant_client
    
    # å‚³é€è¼‰å…¥ä¸­çš„è¨Šæ¯çµ¦ä½¿ç”¨è€…
    loading_msg = cl.Message(content="### âš™ï¸ ç³»çµ±åˆå§‹åŒ–ä¸­... æ­£åœ¨è¼‰å…¥ AI æ¨¡å‹ï¼Œè«‹ç¨å€™ã€‚")
    await loading_msg.send()

    try:
        # è¼‰å…¥æ¨¡å‹ï¼ˆè‹¥å°šæœªè¼‰å…¥ï¼‰
        if llm_llama3 is None:
            llm_llama3 = load_model(MODEL_LLAMA3_PATH)
        if llm_sec is None:
            llm_sec = load_model(MODEL_SEC_PATH)
        if qdrant_client is None:
            print("Connecting to Qdrant instance...")
            qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
            qdrant_client = QdrantClient(url=qdrant_url)
            print("Setting up embedding model...")
            qdrant_client.set_model("BAAI/bge-small-en-v1.5")
            
        loading_msg.content = "### âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\n\nğŸ›¡ï¸ **æ­¡è¿ä½¿ç”¨ Foundation-Sec-8B Security Assistant!** ğŸ›¡ï¸\n\næ‚¨å¯ä»¥é–‹å§‹è¼¸å…¥æœ‰é—œè³‡å®‰ã€ç¨‹å¼è¨­è¨ˆæˆ–ä¸€èˆ¬å•é¡Œã€‚"
        await loading_msg.update()
        
    except Exception as e:
        loading_msg.content = f"### âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—\néŒ¯èª¤è¨Šæ¯: `{e}`\nè«‹ç¢ºèªæ¨¡å‹è·¯å¾‘æ˜¯å¦æ­£ç¢º (é è¨­è·¯å¾‘ `./models/...`)ã€‚"
        await loading_msg.update()
        return

    # åˆå§‹åŒ–é€™å€‹ä½¿ç”¨è€…çš„èŠå¤©æ­·å²ç´€éŒ„
    cl.user_session.set("chat_history", [])

@cl.on_message
async def main(message: cl.Message):
    global llm_llama3, llm_sec, qdrant_client
    
    if llm_llama3 is None or llm_sec is None or qdrant_client is None:
        await cl.Message(content="âš ï¸ æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆï¼Œè«‹é‡æ•´é é¢æˆ–ç¢ºèªçµ‚ç«¯æ©ŸéŒ¯èª¤è¨Šæ¯ã€‚").send()
        return

    user_input = message.content.strip()
    chat_history = cl.user_session.get("chat_history", [])

    # === Intent Classification with Llama3 ===
    classification_messages = [
        {
            "role": "system",
            "content": "You are a specialized technical router. You must classify if the user's input is related to IT, security, programming, system architecture, or operating systems.\n"
                       "Reply with EXACTLY ONE word: 'YES' or 'NO'. Do NOT provide any explanations, code, or repeat the user's input.\n"
                       "Reply 'YES' if the input contains ANY of the following: programming questions, tracebacks, errors, code snippets, system architecture design, operating system queries, Apache logs, Nginx logs, PHP errors, permission denied, SQL injection, hacking, bugs, server crashes, security audit, or any raw code/log output.\n"
                       "Reply 'NO' only if it is a general casual chat like 'Hi', 'How are you', etc."
        }
    ]

    # åƒ…æ ¹æ“šç•¶å‰å•é¡Œåˆ¤æ–· Intentï¼Œé¿å…æ­·å²å°è©±éé•·å°è‡´åˆ†é¡å™¨ï¼ˆLlama 3ï¼‰æ··äº‚è€Œç„¡æ³•æ­£ç¢ºè¼¸å‡º YES/NO
    classification_messages.append({"role": "user", "content": user_input})

    is_security = False
    
    # å»ºç«‹ IT é—œéµå­—å®‰å…¨ç¶²ï¼Œé˜²æ­¢å°å‹æ¨¡å‹å°ç”Ÿç¡¬ Log åˆ†é¡å¤±æ•—
    critical_it_keywords = ["http", "get ", "post ", "error", "exception", "php", "sql", "login", ".bak", "log", "404", "500", "id_rsa", "ssh"]
    user_input_lower = user_input.lower()
    
    # å¦‚æœ Llama3 åˆ¤æ–·éŒ¯èª¤ï¼Œä½†å…§å®¹æ˜é¡¯æ˜¯ IT/Log ç›¸é—œï¼Œå¼·åˆ¶å®šç¾©ç‚ºè³‡å®‰å•é¡Œ
    if any(keyword in user_input_lower for keyword in critical_it_keywords):
        is_security = True
        print("[DEBUG] Intent forced to YES by Keyword Matching")
    else:
        try:
            res = llm_llama3.create_chat_completion(
                messages=classification_messages,
                max_tokens=2,
                temperature=0.0
            )
            intent_text = res["choices"][0]["message"]["content"].strip().upper()
            intent_usage = res.get("usage", {})
            print(f"[DEBUG] Intent Classification = {intent_text} | Tokens: {intent_usage}")
            is_security = "YES" in intent_text
        except Exception as e:
            print(f"[Classification Error]: {e}")
            is_security = False

    # æ ¹æ“šåˆ†é¡çµæœæ±ºå®šä½¿ç”¨çš„æ¨¡å‹
    active_llm = llm_sec if is_security else llm_llama3
    active_name = "Foundation-Sec" if is_security else "Llama3-Taiwan"
    active_system_msg = sec_system_message if is_security else general_system_message

    # === Main Generation ===
    chat_messages = [{"role": "system", "content": active_system_msg}]
    
    # ç„¡è«–æ˜¯ä¸€èˆ¬é‚„æ˜¯è³‡å®‰å•é¡Œï¼Œéƒ½å¸¶å…¥æ­·å²å°è©±ç´€éŒ„ï¼Œç¢ºä¿å¤šè¼ªä¸Šä¸‹æ–‡è¨˜æ†¶
    for msg in chat_history:
        chat_messages.append(msg)

    if not is_security:
        chat_messages.append({"role": "user", "content": user_input})
    else:
        # === Qdrant RAG Context Retrieval ===
        context_str = ""
        try:
            search_result = qdrant_client.query(
                collection_name="security_playbooks",
                query_text=user_input,
                limit=1
            )
            if search_result:
                best_match = search_result[0]
                print(f"[RAG] Found context: {best_match.metadata.get('title')} (score: {best_match.score:.2f})")
                context_str = f"[Internal System Context]\n{best_match.document}\n\n"
        except Exception as e:
            print(f"[RAG Error] {e}")

        # é‡å°è³‡å®‰ç›¸é—œå•é¡Œï¼Œé©åº¦æé†’å›è¦†è‹±æ–‡å³å¯ï¼Œä¸è¦ç”¨èªæ°£éæ–¼å¼·çƒˆçš„å¨è„…æ€§å­—çœ¼ï¼Œé¿å… 8B æ¨¡å‹å¼•ç™¼å¹»è¦ºå´©æ½°
        enforced_input = f"{context_str}{user_input}\n\n[Action: Please analyze the above input and respond in English only. Base your answer on the Internal System Context if it is relevant.]"
        chat_messages.append({"role": "user", "content": enforced_input})

    # æº–å‚™ä¸€å€‹æ˜é¡¯çš„å‰ç¶´æ¨™ç±¤ï¼Œè®“ä½¿ç”¨è€…çŸ¥é“æ˜¯å“ªå€‹æ¨¡å‹åœ¨å›ç­”
    model_badge = f"### ğŸ§  ç”± `{active_name}` ç”Ÿæˆå›æ‡‰\n---\n"
    
    # å…ˆç™¼é€ä¸€å€‹ç©ºçš„ Message (UI å‡ºç¾è¼‰å…¥å‹•ç•«)ï¼Œä¹‹å¾Œæœƒé€æ­¥ä¸²æµ (Stream)
    response_msg = cl.Message(content=model_badge, author=active_name)
    await response_msg.send()

    assistant_response = ""
    
    try:
        stream = active_llm.create_chat_completion(
            messages=chat_messages,
            stream=True,         
            temperature=0.4 if is_security else 0.2,  # æé«˜è‡ªç„¶è®ŠåŒ–ç‡ä»¥å–ä»£å¼·å£“å¼çš„æ‡²ç½°
            top_p=0.9,
            repeat_penalty=1.05 if is_security else 1.0, # é™åˆ°æœ€åº•ç·šï¼Œé˜²æ­¢ logits å´©æ½°æˆäº‚ç¢¼
            frequency_penalty=0.0, # å…¨é¢é—œé–‰ï¼Œé€™æ˜¯å°è‡´å°å‡ºå¥‡æ€ªç¬¦è™Ÿçš„ä¸»å› 
            presence_penalty=0.0,  # å…¨é¢é—œé–‰
            max_tokens=600 if is_security else 2048,
            stop=["<|eot_id|>", "<|end_of_text|>", "</s>", "[INST]", "User:", "[Foundation-Sec]:", "\n\n", "Your response:"]
        )

        usage_main = None
        for chunk in stream:
            if "usage" in chunk and chunk["usage"]:
                usage_main = chunk["usage"]
            if "choices" in chunk and len(chunk["choices"]) > 0:
                delta = chunk["choices"][0].get("delta", {})
                if "content" in delta:
                    text_chunk = delta["content"]
                    assistant_response += text_chunk
                    await response_msg.stream_token(text_chunk) # å³æ™‚å°‡å­—ä¸²é€å¾€å‰ç«¯
                    
        # è¨ˆç®— Tokens è³‡è¨Š
        if not usage_main:
            p_tokens = len(active_llm.tokenize(str(chat_messages).encode("utf-8")))
            c_tokens = len(active_llm.tokenize(assistant_response.encode("utf-8")))
            usage_main = {"prompt_tokens": p_tokens, "completion_tokens": c_tokens, "total_tokens": p_tokens + c_tokens}
            
        token_info_main = f"\n\n---\n*âš¡ Tokens: {usage_main.get('total_tokens', 0)} (è¼¸å…¥: {usage_main.get('prompt_tokens', 0)} | è¼¸å‡º: {usage_main.get('completion_tokens', 0)})*"
        await response_msg.stream_token(token_info_main)
        
        await response_msg.update() # çµæŸ Token ä¸²æµ

        # === Translation for Security Output ===
        if is_security:
            trans_badge = f"### ğŸ§  ç”± `Llama3-Taiwan` é€²è¡Œç¿»è­¯\n---\n"
            trans_msg = cl.Message(content=f"\n\n> ğŸ”„ *æ­£åœ¨å‘¼å« Llama3-Taiwan ç¿»è­¯æˆä¸­æ–‡...*\n\n", author="Translator")
            await trans_msg.send()
            
            trans_messages = [
                {"role": "system", "content": "ä½ æ˜¯è³‡å®‰ç¿»è­¯å°ˆå®¶ã€‚è«‹å°‡åŸæ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚è«‹ç›´æ¥è¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŠ ä¸Šä»»ä½•è§£é‡‹æˆ–é–‹å ´ç™½ã€‚"},
                {"role": "user", "content": f"åŸæ–‡ï¼š\n{assistant_response}"},
            ]
            
            chinese_response = ""
            try:
                trans_stream = llm_llama3.create_chat_completion(
                    messages=trans_messages,
                    stream=True,
                    temperature=0.1,
                    max_tokens=600,
                    stop=["<|eot_id|>", "<|end_of_text|>"]
                )
                
                # é‡è¨­ç¿»è­¯å€å¡Šçš„å…§å®¹æº–å‚™æ¥æ”¶æ–°çš„ä¸²æµï¼Œä¸¦åŠ ä¸Šæ¨¡å‹æ¨™ç±¤
                trans_msg.content = trans_badge
                await trans_msg.update()

                trans_usage = None
                for chunk in trans_stream:
                    if "usage" in chunk and chunk["usage"]:
                        trans_usage = chunk["usage"]
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0].get("delta", {})
                        if "content" in delta:
                            text_chunk = delta["content"]
                            chinese_response += text_chunk
                            await trans_msg.stream_token(text_chunk)
                
                # è¨ˆç®— Tokens è³‡è¨Š
                if not trans_usage:
                    tp_tokens = len(llm_llama3.tokenize(str(trans_messages).encode("utf-8")))
                    tc_tokens = len(llm_llama3.tokenize(chinese_response.encode("utf-8")))
                    trans_usage = {"prompt_tokens": tp_tokens, "completion_tokens": tc_tokens, "total_tokens": tp_tokens + tc_tokens}
                
                trans_token_info = f"\n\n---\n*âš¡ Tokens: {trans_usage.get('total_tokens', 0)} (è¼¸å…¥: {trans_usage.get('prompt_tokens', 0)} | è¼¸å‡º: {trans_usage.get('completion_tokens', 0)})*"
                await trans_msg.stream_token(trans_token_info)
                
                await trans_msg.update()
                
                # æ³¨æ„ï¼šé€™è£¡ã€Œä¸ã€å°‡ä¸­æ–‡ç¿»è­¯çµæœæ•´ä½µé€² assistant_response
                # é€™æ¨£æ‰èƒ½ç¢ºä¿è³‡å®‰æ¨¡å‹ï¼ˆåªèƒ½è¬›è‹±æ–‡ï¼‰åœ¨è®€å–æ­·å²ç´€éŒ„æ™‚ï¼Œä¸æœƒçœ‹åˆ°è‡ªå·±ç”¢ç”Ÿä¸­æ–‡ï¼Œé¿å…ç™¼ç”Ÿèªç³»å¹»è¦ºæ±¡æŸ“
                
            except Exception as e:
                print(f"[Translation Error]: {e}")
                trans_msg.content = "**[ä¸­æ–‡ç¿»è­¯å¤±æ•—]**\n" + str(e)
                await trans_msg.update()

        # Update chat history (åªå­˜ä¹¾æ·¨çš„è‹±æ–‡æˆ–ä¸€èˆ¬å°è©±ï¼Œä¸å« Token è³‡è¨Šèˆ‡ç¿»è­¯)
        chat_history.append({"role": "user", "content": user_input})
        chat_history.append({"role": "assistant", "content": assistant_response})
        cl.user_session.set("chat_history", chat_history)

    except Exception as e:
        error_msg = f"âŒ ç”¢ç”Ÿå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        await cl.Message(content=error_msg, author="System").send()
