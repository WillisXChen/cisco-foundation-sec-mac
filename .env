# InfluxDB v3 Configuration
INFLUXDB_URL=http://localhost:8181
INFLUXDB_TOKEN=apiv3_cisco-super-secret-auth-token
INFLUXDB_ORG=cisco
INFLUXDB_BUCKET=metrics

# Qdrant Configuration
QDRANT_URL=http://localhost:6333

# Model Paths
MODEL_SEC_PATH=./models/foundation-sec-8b-q4_k_m.gguf
MODEL_LLAMA3_PATH=./models/llama-3-taiwan-8b-instruct-q4_k_m.gguf

# Langfuse Configuration (Self-Hosted on Docker Compose)
LANGFUSE_HOST=http://localhost:3001
LANGFUSE_SECRET_KEY=sk-lf-1234567890
LANGFUSE_PUBLIC_KEY=pk-lf-1234567890

# GPU Layer Configuration (-1 = Auto/Metal, 0 = CPU, N = Number of layers to offload)
# For Llama 3 8B, 33 is full GPU. Set to -1 for maximum Mac performance.
N_GPU_LAYERS_LLAMA3=5
N_GPU_LAYERS_SEC=15

# Memory Optimization (Context Size)
# Reducing from 4096 to 2048 saves ~1GB RAM per model.
# Total RAM with two 8B models + 2048 ctx will be ~11GB (just under 50% of 24GB).
N_CTX_LLAMA3=2048
N_CTX_SEC=2048
