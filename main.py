import asyncio
import os
import engineio
import chainlit as cl
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from langfuse import observe

# Import our separated modules
from core.i18n import _t, get_lang_name
from core.config import (
    MODEL_SEC_PATH, MODEL_LLAMA3_PATH, PLAYBOOKS_PATH
)
from core.logger import logger
from langfuse import Langfuse
import core.services as services

# Explicitly initialize Langfuse to ensure host matches
langfuse_client = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST")
)
# Ensure API routes are loaded
import api

# Performance optimization for Large Payloads
engineio.payload.Payload.max_decode_packets = 500000
os.makedirs(".files", exist_ok=True)

# --- Chainlit Callbacks ---
@cl.on_chat_start
async def on_chat_start():
    # Attempt to extract 'lang' from URL parameters
    referer = cl.context.session.environ.get("HTTP_REFERER")
    lang_param = cl.user_session.get("language") or "en"
    if referer:
        try:
            from urllib.parse import urlparse, parse_qs
            parsed_url = urlparse(referer)
            query_params = parse_qs(parsed_url.query)
            if "lang" in query_params:
                lang_param = query_params["lang"][0]
        except Exception as e:
            logger.warning(f"Failed to parse lang from referer: {e}")
            
    cl.user_session.set("lang", lang_param)
    
    desc = _t("View Hardware History", lang=lang_param)
    actions = [cl.Action(name="view_hw_history", payload={"action": "show"}, description=desc)]
    
    # Start background tasks
    services.start_hardware_monitor()

    init_msg = _t("### ‚öôÔ∏è System Initializing...", lang=lang_param)
    loading_msg = cl.Message(content=init_msg, actions=actions, author="System")
    await loading_msg.send()

    try:
        # Load models if not already loaded
        for step, (name, path, loader) in enumerate([
            ("Llama3-Taiwan", MODEL_LLAMA3_PATH, services.llm_manager.load_general_model),
            ("Foundation-Sec", MODEL_SEC_PATH, services.llm_manager.load_security_model)
        ], 1):
            msg = _t("### ‚öôÔ∏è Loading ({step}/4): {name}...", lang=lang_param, step=step, name=name)
            loading_msg.content = msg
            await loading_msg.update()
            await asyncio.to_thread(loader, path)

        msg = _t("### ‚öôÔ∏è Loading (3/4): Initializing Vector Database...", lang=lang_param)
        loading_msg.content = msg
        await loading_msg.update()
        await asyncio.to_thread(services.vector_db.setup_model)

        msg = _t("### ‚öôÔ∏è Loading (4/4): Syncing Knowledge Base...", lang=lang_param)
        loading_msg.content = msg
        await loading_msg.update()
        # Always run ingestion to sync any changes in playbooks.json
        await asyncio.to_thread(services.vector_db.ingest_playbooks, PLAYBOOKS_PATH)

        msg = _t("### ‚úÖ System Ready!\nüõ°Ô∏è **Foundation-Sec-8B Security Assistant** Started.", lang=lang_param)
        loading_msg.content = msg
        await loading_msg.update()
    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        msg = _t("### ‚ùå Initialization Failed: `{e}`", lang=lang_param, e=e)
        loading_msg.content = msg
        await loading_msg.update()

    cl.user_session.set("chat_history", [])

@cl.on_message
@observe()
async def main(message: cl.Message):
    chat_history = cl.user_session.get("chat_history", [])
    user_input = message.content.strip()
    lang = cl.user_session.get("lang", "en")
    target_lang_name = get_lang_name(lang)

    # Main Response Generation
    response_msg = cl.Message(content="", author="System")
    assistant_full_text = ""
    is_sec = False

    # Start Phoenix Trace with Hardware context
    with services.tracer.start_as_current_span(f"Chat Generation: {user_input[:20]}...") as span:
        # Get current hardware snapshot
        hw_stats = services._latest_hw_stats_ref
        if hw_stats:
            span.set_attribute("hw.gpu_pct", hw_stats.get("gpu_pct", 0))
            span.set_attribute("hw.ram_pct", hw_stats.get("ram_pct", 0))
            span.set_attribute("hw.total_power_w", hw_stats.get("total_power_w", 0))

        async for chunk in services.assistant_service.generate_response(user_input, chat_history, target_lang=target_lang_name):
            if chunk["type"] == "meta":
                response_msg.author = chunk["author"]
                msg = _t("### üß† Generated by `{author}`\n---\n", lang=lang, author=chunk["author"])
                response_msg.content = msg
                is_sec = chunk["is_security"]
                span.set_attribute("llm.author", chunk["author"])
                await response_msg.send()
            elif chunk["type"] == "token":
                assistant_full_text += chunk["content"]
                await response_msg.stream_token(chunk["content"])
            elif chunk["type"] == "final":
                in_label = _t("In", lang=lang)
                out_label = _t("Out", lang=lang)
                token_info = (
                    f"\n\n---\n*‚ö° Tokens: {chunk['tokens']['total']} "
                    f"({in_label}: {chunk['tokens']['prompt']} | {out_label}: {chunk['tokens']['completion']}) "
                    f"¬∑ üïê {chunk['elapsed']:.1f}s*"
                )
                await response_msg.stream_token(token_info)
                await response_msg.update()

    # Language Detection Utility
    def contains_chinese(text):
        return any('\u4e00' <= char <= '\u9fff' for char in text)

    # Decision logic: Only translate if security response is NOT in the user's target language
    needs_translation = False
    if is_sec and target_lang_name != "English":
        needs_translation = True

    if needs_translation:
        trans_msg = cl.Message(content=_t("\n\n> üîÑ *Translating...*\n\n", lang=lang), author="Translator")
        await trans_msg.send()
        trans_full_text = ""
        
        async for chunk in services.assistant_service.translate_response(assistant_full_text, target_lang_name):
            if chunk["type"] == "meta":
                trans_msg.content = _t("### üß† Translated by `{author}`\n---\n", lang=lang, author="Llama3-Taiwan")
                await trans_msg.update()
            elif chunk["type"] == "token":
                trans_full_text += chunk["content"]
                await trans_msg.stream_token(chunk["content"])
            elif chunk["type"] == "final":
                token_info = (
                    f"\n\n---\n*‚ö° Tokens: {chunk['tokens']['total']} "
                    f"¬∑ üïê {chunk['elapsed']:.1f}s*"
                )
                await trans_msg.stream_token(token_info)
                await trans_msg.update()

    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": assistant_full_text})
    cl.user_session.set("chat_history", chat_history)
    
    # Force flush Langfuse data to ensure it's sent before function ends
    try:
        services.langfuse_client.flush()
    except Exception as e:
        logger.error(f"Langfuse flush failed: {e}")


@cl.action_callback("view_hw_history")
async def on_action_view_hw_history(action: cl.Action):
    lang = cl.user_session.get("lang", "en")

    msg = _t("üìä Fetching historical data from InfluxDB...", lang=lang)
    await cl.Message(content=msg, author="System").send()
    
    try:
        if services.metrics_db is None:
            msg = _t("‚ö†Ô∏è Database not connected, please try again later.", lang=lang)
            await cl.Message(content=msg, author="System").send()
            return

        df = await asyncio.to_thread(services.metrics_db.query_hardware_history_df)
        if df.empty:
            msg = _t("‚ö†Ô∏è Not enough historical data collected yet.", lang=lang)
            await cl.Message(content=msg, author="System").send()
            return

        titles = (_t("Usage (%)", lang=lang), _t("Power (Watt)", lang=lang))
        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=titles)
        for col, name in [('e_cpu_pct', 'E-CPU %'), ('p_cpu_pct', 'P-CPU %'), ('gpu_pct', 'GPU %'), ('ram_pct', 'RAM %')]:
            if col in df.columns: fig.add_trace(go.Scatter(x=df['_time'], y=df[col], name=name), row=1, col=1)
        for col, name in [('cpu_power_w', 'CPU W'), ('gpu_power_w', 'GPU W'), ('total_power_w', 'Total W')]:
            if col in df.columns: fig.add_trace(go.Scatter(x=df['_time'], y=df[col], name=name), row=2, col=1)

        title_text = _t("Last 15 Minutes Trend", lang=lang)
        fig.update_layout(
            height=650, 
            template="plotly_dark", 
            title_text=title_text,
            legend=dict(
                orientation="h",
                yanchor="top",
                y=-0.15,
                xanchor="center",
                x=0.5
            ),
            margin=dict(b=80)
        )
        msg = _t("‚úÖ **History Chart Generated**", lang=lang)
        chart_name = _t("History Monitor", lang=lang)
        await cl.Message(content=msg, elements=[cl.Plotly(chart_name, figure=fig, display="inline")], author="Monitor").send()
    except Exception as e:
        logger.error(f"Plot error: {e}")
        msg = _t("‚ùå Data Read Error: {e}", lang=lang, e=e)
        await cl.Message(content=msg, author="System").send()
