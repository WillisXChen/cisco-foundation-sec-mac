import asyncio
import os
import engineio
import chainlit as cl
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from langfuse import observe

# Import our separated modules
from core.config import (
    MODEL_SEC_PATH, MODEL_LLAMA3_PATH, PLAYBOOKS_PATH
)
from core.logger import logger
import core.services as services
# Ensure API routes are loaded
import api

# Performance optimization for Large Payloads
engineio.payload.Payload.max_decode_packets = 500000
os.makedirs(".files", exist_ok=True)

# --- Chainlit Callbacks ---
@cl.on_chat_start
async def on_chat_start():
    actions = [cl.Action(name="view_hw_history", payload={"action": "show"}, description="æŸ¥çœ‹æ­·å²è³‡æºè¶¨å‹¢")]
    
    # Start background tasks
    services.start_hardware_monitor()

    loading_msg = cl.Message(content="### âš™ï¸ ç³»çµ±åˆå§‹åŒ–ä¸­...", actions=actions)
    await loading_msg.send()

    try:
        # Load models if not already loaded
        for step, (name, path, loader) in enumerate([
            ("Llama3-Taiwan", MODEL_LLAMA3_PATH, services.llm_manager.load_general_model),
            ("Foundation-Sec", MODEL_SEC_PATH, services.llm_manager.load_security_model)
        ], 1):
            loading_msg.content = f"### âš™ï¸ è¼‰å…¥ä¸­ ({step}/4)ï¼šæ­£åœ¨è¼‰å…¥ {name}..."
            await loading_msg.update()
            await asyncio.to_thread(loader, path)

        loading_msg.content = "### âš™ï¸ è¼‰å…¥ä¸­ (3/4)ï¼šæ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«..."
        await loading_msg.update()
        await asyncio.to_thread(services.vector_db.setup_model)

        loading_msg.content = "### âš™ï¸ è¼‰å…¥ä¸­ (4/4)ï¼šåŒæ­¥çŸ¥è­˜åº«..."
        await loading_msg.update()
        if not await asyncio.to_thread(services.vector_db.is_collection_exists):
            await asyncio.to_thread(services.vector_db.ingest_playbooks, PLAYBOOKS_PATH)

        loading_msg.content = "### âœ… ç³»çµ±å°±ç·’ï¼\nğŸ›¡ï¸ **Foundation-Sec-8B Security Assistant** å·²å•Ÿå‹•ã€‚"
        await loading_msg.update()
    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        loading_msg.content = f"### âŒ åˆå§‹åŒ–å¤±æ•—: `{e}`"
        await loading_msg.update()

    cl.user_session.set("chat_history", [])

@cl.on_message
@observe()
async def main(message: cl.Message):
    chat_history = cl.user_session.get("chat_history", [])
    user_input = message.content.strip()

    # Main Response Generation
    response_msg = cl.Message(content="", author="System")
    assistant_full_text = ""
    is_sec = False

    # Start Phoenix Trace
    with services.tracer.start_as_current_span(f"Chat Generation: {user_input[:20]}..."):
        async for chunk in services.assistant_service.generate_response(user_input, chat_history):
            if chunk["type"] == "meta":
                response_msg.author = chunk["author"]
                response_msg.content = f"### ğŸ§  ç”± `{chunk['author']}` ç”Ÿæˆå›æ‡‰\n---\n"
                is_sec = chunk["is_security"]
                await response_msg.send()
            elif chunk["type"] == "token":
                assistant_full_text += chunk["content"]
                await response_msg.stream_token(chunk["content"])
            elif chunk["type"] == "final":
                token_info = (
                    f"\n\n---\n*âš¡ Tokens: {chunk['tokens']['total']} "
                    f"(é€²: {chunk['tokens']['prompt']} | å‡º: {chunk['tokens']['completion']}) "
                    f"Â· ğŸ• {chunk['elapsed']:.1f}s*"
                )
                await response_msg.stream_token(token_info)
                await response_msg.update()

    # Optional Translation
    if is_sec:
        trans_msg = cl.Message(content="\n\n> ğŸ”„ *æ­£åœ¨ç¿»è­¯å›ä¸­æ–‡...*\n\n", author="Translator")
        await trans_msg.send()
        trans_full_text = ""
        
        async for chunk in services.assistant_service.translate_response(assistant_full_text):
            if chunk["type"] == "meta":
                trans_msg.content = f"### ğŸ§  ç”± `Llama3-Taiwan` é€²è¡Œç¿»è­¯\n---\n"
                await trans_msg.update()
            elif chunk["type"] == "token":
                trans_full_text += chunk["content"]
                await trans_msg.stream_token(chunk["content"])
            elif chunk["type"] == "final":
                token_info = (
                    f"\n\n---\n*âš¡ Tokens: {chunk['tokens']['total']} "
                    f"Â· ğŸ• {chunk['elapsed']:.1f}s*"
                )
                await trans_msg.stream_token(token_info)
                await trans_msg.update()

    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": assistant_full_text})
    cl.user_session.set("chat_history", chat_history)

@cl.action_callback("view_hw_history")
async def on_action_view_hw_history(action: cl.Action):
    await cl.Message(content="ğŸ“Š æ­£åœ¨å¾ InfluxDB æå–æ­·å²æ•¸æ“š...", author="System").send()
    
    try:
        if services.metrics_db is None:
            await cl.Message(content="âš ï¸ è³‡æ–™åº«æœªé€£ç·šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", author="System").send()
            return

        df = await asyncio.to_thread(services.metrics_db.query_hardware_history_df)
        if df.empty:
            await cl.Message(content="âš ï¸ å°šæœªæ”¶é›†åˆ°è¶³å¤ çš„æ­·å²æ•¸æ“šã€‚", author="System").send()
            return

        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=("ä½¿ç”¨ç‡ (%)", "é›»åŠ› (Watt)"))
        for col, name in [('e_cpu_pct', 'E-CPU %'), ('p_cpu_pct', 'P-CPU %'), ('gpu_pct', 'GPU %'), ('ram_pct', 'RAM %')]:
            if col in df.columns: fig.add_trace(go.Scatter(x=df['_time'], y=df[col], name=name), row=1, col=1)
        for col, name in [('cpu_power_w', 'CPU W'), ('gpu_power_w', 'GPU W'), ('total_power_w', 'Total W')]:
            if col in df.columns: fig.add_trace(go.Scatter(x=df['_time'], y=df[col], name=name), row=2, col=1)

        fig.update_layout(
            height=650, 
            template="plotly_dark", 
            title_text="æœ€è¿‘ 15 åˆ†é˜è¶¨å‹¢",
            legend=dict(
                orientation="h",
                yanchor="top",
                y=-0.15,
                xanchor="center",
                x=0.5
            ),
            margin=dict(b=80)
        )
        await cl.Message(content="âœ… **æ­·å²åœ–è¡¨å·²ç”Ÿæˆ**", elements=[cl.Plotly("æ­·å²ç›£æ§", figure=fig, display="inline")], author="H/W Monitor").send()
    except Exception as e:
        logger.error(f"Plot error: {e}")
        await cl.Message(content=f"âŒ è®€å–æ•¸æ“šéŒ¯èª¤: {e}", author="System").send()
